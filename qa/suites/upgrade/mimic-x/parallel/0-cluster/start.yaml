meta:
- desc: |
   Run ceph on two nodes,
   with a separate client 0,1,2 third node.
   Use xfs beneath the osds.
   CephFS tests running on client 2,3
   #Note-To enable RHEL runs on ovh nodes, add the following to overrides
   #ansible.cephlab:
   # skip_tags: entitlements,packages,repos
roles:
- - mon.a
  - mgr.x
  - mds.a
  - osd.0
  - osd.1
- - mon.b
  - mgr.y
  - osd.2
  - osd.3
- - mon.c
  - mgr.z
  - mds.b
  - osd.4
  - osd.5
- - client.0
  - client.1
  - client.2
  - client.3
overrides:
  ceph:
    mon_bind_msgr2: false
    mon_bind_addrvec: false
    log-whitelist:
    - scrub mismatch
    - ScrubResult
    - wrongly marked
    - \(POOL_APP_NOT_ENABLED\)
    - \(SLOW_OPS\)
    - overall HEALTH_
    - \(MON_MSGR2_NOT_ENABLED\)
    - overall HEALTH_
    - \(MGR_DOWN\)
    - \(PG_
    - replacing it with standby
    - No standby daemons available
    - \(FS_DEGRADED\)
    - \(MDS_FAILED\)
    - \(MDS_DEGRADED\)
    - \(FS_WITH_FAILED_MDS\)
    - \(MDS_DAMAGE\)
    - \(MDS_ALL_DOWN\)
    - \(MDS_UP_LESS_THAN_MAX\)
    - \(OSD_DOWN\)
    - \(OSD_HOST_DOWN\)
    - \(POOL_APP_NOT_ENABLED\)
    - pauserd,pausewr flag\(s\) set
    - Monitor daemon marked osd\.[[:digit:]]+ down, but it is still running
    conf:
      global:
        enable experimental unrecoverable data corrupting features: "*"
      mon:
        mon warn on osd down out interval zero: false
      osd:
        osd_class_load_list: "*"
        osd_class_default_list: "*"
    fs: xfs
